{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",temperature=0.9,max_tokens=500\n",
    ")\n",
    "\n",
    "@tool\n",
    "def SqlQueryWritter(user_input : str)->str:\n",
    "    \"\"\"\n",
    "    A tool that generates SQL queries based on user input.\n",
    "    \"\"\"\n",
    "   \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\"you are an helpfull assistent to write the SQL Queries\"),\n",
    "        \n",
    "            (\"user\",\"{query}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "\n",
    "    response = chain.invoke({\"query\":user_input})\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def SqlQuerySummarizer(user_input : str)->str:\n",
    "    \"\"\"\n",
    "    A tool that generates SQL queries based on user input.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\"you are an helpfull assistent , Summarize the SQL Queries\"),\n",
    "        \n",
    "            (\"user\",\"{query}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "\n",
    "    response = chain.invoke({\"query\":user_input})\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[SqlQueryWritter,SqlQuerySummarizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools\n",
    "#tools = [SqlQueryWritter]\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "# Create the prompt for the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful assistant to write queries or summarize queries\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.agents.initialize import initialize_agent\n",
    "#from langchain.agents import create_react_agent\n",
    "\n",
    "\n",
    "from langchain.agents import load_tools                                         \n",
    "from langchain.agents import initialize_agent\n",
    "# Create the agent using the tools\n",
    "#agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "chat_history = []\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    prompt=prompt \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "llm_withtools=llm.bind_tools(tools)\n",
    "agent=(\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_withtools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "#memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",input_key=\"input\",output_key='output', return_messages=True, k=4)\n",
    "\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=agent_executor.invoke({\"input\":\"top 20 records sql sql query\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To retrieve the top 20 records from a database table using SQL, you can use different SQL syntax depending on the database you are using. Here are some examples:\n",
      "\n",
      "### For MySQL, PostgreSQL, SQLite\n",
      "```sql\n",
      "SELECT *\n",
      "FROM your_table_name\n",
      "ORDER BY some_column\n",
      "LIMIT 20;\n",
      "```\n",
      "\n",
      "### For SQL Server\n",
      "```sql\n",
      "SELECT TOP 20 *\n",
      "FROM your_table_name\n",
      "ORDER BY some_column;\n",
      "```\n",
      "\n",
      "### For Oracle\n",
      "Oracle doesn't use `LIMIT` or `TOP`. Instead, you can use:\n",
      "\n",
      "#### Using `FETCH` Clause\n",
      "```sql\n",
      "SELECT *\n",
      "FROM your_table_name\n",
      "ORDER BY some_column\n",
      "FETCH FIRST 20 ROWS ONLY;\n",
      "```\n",
      "\n",
      "#### Using `ROWNUM`\n",
      "```sql\n",
      "SELECT *\n",
      "FROM (\n",
      "  SELECT *\n",
      "  FROM your_table_name\n",
      "  ORDER BY some_column\n",
      ")\n",
      "WHERE ROWNUM <= 20;\n",
      "```\n",
      "\n",
      "Ensure to replace `your_table_name` with the actual table name and `some_column` with the column you wish to order the records by. The `ORDER BY` clause is crucial to define which records are considered the \"top\" ones.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Method Function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime,timedelta\n",
    "from openai import OpenAI\n",
    "\n",
    "openai=OpenAI()\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_delivery_date\",\n",
    "            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Delivery Date.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"date\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def get_delivery_date(date :str):\n",
    "    \"\"\"Get flight information between two locations.\"\"\"\n",
    "    delivery_date = datetime.strptime(date, \"%Y-%m-%d\") + timedelta(days=3)\n",
    "\n",
    "    return json.dumps({\"date\":delivery_date.strftime(\"%Y-%m-%d\")})\n",
    "    \n",
    "\n",
    "\n",
    "user_prompt = \"Get my phone delivary date?\"\n",
    "\n",
    "\n",
    "output = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    tools=tools\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_bSJJiGCtx61mS74fiprUKWXS', function=Function(arguments='{\"date\":\"2023-10-25\"}', name='get_delivery_date'), type='function')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = output.choices[0].message.tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-25'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(tool_call.function.arguments).get(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_YuSIwSKhnSIkm05WsDtmKQew', function=Function(arguments='{\"date\":\"2023-10-25\"}', name='get_delivery_date'), type='function')])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletion' object has no attribute 'function_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m origin \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241m.\u001b[39marguments)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_origin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m destination \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(output\u001b[38;5;241m.\u001b[39mfunction_call\u001b[38;5;241m.\u001b[39marguments)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_destination\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(output\u001b[38;5;241m.\u001b[39mfunction_call\u001b[38;5;241m.\u001b[39marguments)\n",
      "File \u001b[1;32mc:\\Users\\arung\\anaconda3\\envs\\RagLLM\\lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatCompletion' object has no attribute 'function_call'"
     ]
    }
   ],
   "source": [
    "origin = json.loads(output.function_call.arguments).get(\"loc_origin\")\n",
    "destination = json.loads(output.function_call.arguments).get(\"loc_destination\")\n",
    "params = json.loads(output.function_call.arguments)\n",
    "type(params)\n",
    "\n",
    "print(origin)\n",
    "print(destination)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# # Set up Azure OpenAI credentials\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_key = \"<your_azure_openai_key>\"\n",
    "# openai.api_base = \"https://<your-resource-name>.openai.azure.com/\"\n",
    "# openai.api_version = \"2023-10-01\"\n",
    "\n",
    "# Define the two functions: LLM-based answering and Vector Search\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"llm_generate_answer\",\n",
    "        \"description\": \"Use LLM to attempt to answer the query based on its knowledge\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"question\": {\"type\": \"string\", \"description\": \"The user question to be answered by the LLM\"}\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"vector_search\",\n",
    "        \"description\": \"Perform a vector search to retrieve relevant information from a database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"The query to search in the vector database\"},\n",
    "                \"top_k\": {\"type\": \"integer\", \"description\": \"The number of top results to retrieve\", \"default\": 3}\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Simulate LLM answer generation function\n",
    "def llm_generate_answer(question):\n",
    "    prompt = f\"Answer the following question: {question}\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    \n",
    "    llm_answer = response['choices'][0]['message']['content']\n",
    "    return llm_answer\n",
    "\n",
    "# Simulate vector search function (e.g., from Pinecone, Weaviate, or other vector DBs)\n",
    "def vector_search(query, top_k=3):\n",
    "    # Normally, you'd connect to a real vector database, but we'll simulate results here\n",
    "    vector_results = [\n",
    "        {\"content\": \"Specific result 1 from vector database, related to personal data.\"},\n",
    "        {\"content\": \"Specific result 2 from vector database.\"},\n",
    "        {\"content\": \"Specific result 3 providing deeper context.\"},\n",
    "    ]\n",
    "    return vector_results[:top_k]\n",
    "\n",
    "# Main function to handle simultaneous function calling for LLM and vector search\n",
    "def handle_parallel_calls(prompt):\n",
    "    # Step 1: Trigger both LLM answer generation and vector search simultaneously\n",
    "\n",
    "    # First, call the LLM-based answering function\n",
    "    llm_response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        functions=[{\n",
    "            \"name\": \"llm_generate_answer\",\n",
    "            \"description\": \"Use LLM to answer the query based on its knowledge\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\"type\": \"string\", \"description\": \"The user question to be answered by the LLM\"}\n",
    "                },\n",
    "                \"required\": [\"question\"]\n",
    "            }\n",
    "        }],\n",
    "        function_call=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Simultaneously, call the vector search function\n",
    "    vector_response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Search vector database for: {prompt}\"}],\n",
    "        functions=[{\n",
    "            \"name\": \"vector_search\",\n",
    "            \"description\": \"Search relevant information in the vector database\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The query string.\"},\n",
    "                    \"top_k\": {\"type\": \"integer\", \"description\": \"Number of results to return.\", \"default\": 3}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }],\n",
    "        function_call=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Collect the responses\n",
    "    llm_message = llm_response['choices'][0]['message']\n",
    "    vector_message = vector_response['choices'][0]['message']\n",
    "\n",
    "    llm_answer = \"\"\n",
    "    vector_results = \"\"\n",
    "\n",
    "    # Handle LLM function call\n",
    "    if \"function_call\" in llm_message:\n",
    "        function_name = llm_message['function_call']['name']\n",
    "        arguments = json.loads(llm_message['function_call']['arguments'])\n",
    "\n",
    "        if function_name == \"llm_generate_answer\":\n",
    "            question = arguments['question']\n",
    "            llm_answer = llm_generate_answer(question)\n",
    "\n",
    "    # Handle vector search function call\n",
    "    if \"function_call\" in vector_message:\n",
    "        function_name = vector_message['function_call']['name']\n",
    "        arguments = json.loads(vector_message['function_call']['arguments'])\n",
    "\n",
    "        if function_name == \"vector_search\":\n",
    "            query = arguments['query']\n",
    "            vector_results = vector_search(query, arguments.get('top_k', 3))\n",
    "\n",
    "    # Step 3: Combine results from both LLM and vector search\n",
    "    final_response = llm_answer\n",
    "\n",
    "    if vector_results:\n",
    "        final_response += \"\\n\\nAdditional Information from Vector Search:\\n\"\n",
    "        for result in vector_results:\n",
    "            final_response += result['content'] + \"\\n\"\n",
    "\n",
    "    # Print or return the combined response\n",
    "    print(f\"Final Combined Response:\\n{final_response}\")\n",
    "    return final_response\n",
    "\n",
    "# Example usage\n",
    "user_input = \"Tell me about my recent purchases or trends in the last quarter.\"\n",
    "handle_parallel_calls(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Endless calls repeat,  \\nEach layer peels the next one—  \\nDepth in code unfolds.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RagLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
